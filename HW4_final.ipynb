{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7346b726",
   "metadata": {},
   "source": [
    "<div align=\"center\" dir=\"rtl\">\n",
    "<h2>تمرین چهارم درس بازیابی پیشرفته‌ی اطلاعات</h2>\n",
    "<h3>گروه سی‌و‌سوم: علی مهربانی، پانیذ حلواچی، آرمان سلیمانی</h3>\n",
    "</div>\n",
    "<div align=\"right\" dir=\"rtl\">\n",
    "  <br>\n",
    "    در این تمرین، تسک دسته‌بندی روی داده‌های گرد‌آوری شده از بوستان سعدی انجام می‌شود. ابتدا دیتاست مورد نیازمان از روی داده‌های خام که برای تمرین سوم جمع شده بود ساخته می‌شود، سپس قسمت پیش‌پردازش قرار دارد و در ادامه تسک روی داده‌ها انجام می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1bf1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت اول: آماده‌سازی دیتاست</h3>\n",
    "  <br>\n",
    "    در این قسمت فایل‌های داده‌های خام خوانده شده و دیتاست بیت به بیت برای کلاس‌بندی ایجاد می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c48ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: nltk==3.4 in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hazm) (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk==3.4->hazm) (1.16.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk==3.4->hazm) (3.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchtext) (4.62.3)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchtext) (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchtext) (1.19.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch==1.11.0->torchtext) (4.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->torchtext) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->torchtext) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->torchtext) (2022.5.18.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#import libraries for dataset generation\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "!pip install hazm\n",
    "!pip install torchtext\n",
    "from hazm import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2eabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = listdir('Boostan/')\n",
    "\n",
    "i = 0\n",
    "df = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "\n",
    "for p in all_files:\n",
    "    f = open('Boostan/' + p, 'r', encoding='utf8')\n",
    "    \n",
    "    has_poems = True\n",
    "    \n",
    "    while has_poems:\n",
    "        poem = ''\n",
    "        \n",
    "        while True:\n",
    "            s = f.readline()\n",
    "            if '$' in s:\n",
    "                has_poems = False\n",
    "                break\n",
    "            elif '_' in s:\n",
    "                break\n",
    "            \n",
    "            #age injaiim yani s ye mesra boode\n",
    "            beyt = s + f.readline()[:-1]\n",
    "            df.loc[i] = [beyt, p[:-4]]\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3783c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری این قول معقول نیست\n",
      "چو قانع شدی سیم و سنگت یکی است\n",
      "باب ششم در قناعت\n",
      "3845\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178f294",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت دوم: پیش‌پردازش</h3>\n",
    "  <br>\n",
    "    در این قسمت یک نمونه‌ی پیش‌پردازش شده از دیتاست تهیه می‌شود. دیتاست پیش‌پردازش‌نشده هم در کنار آن حفظ می‌شود که امکان مقایسه‌ی نتیجه روی دو دیتاست وجود داشته باشد.\n",
    "    توضیحات دقیق این که هر گام از پیش‌پردازش شامل چه مواردی است، در تمرین سوم آورده شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0013c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for preprocessing\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eeef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac674b8",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی اول پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، برخی موارد در واژگان (مانند «گه» که همان «گاه» است) اصلاح می‌شوند. عملیات نرمالایز کردن و لمتایز کردن هم انجام داده می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e4ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def preprocess_1(beyt):\n",
    "    beyt = normalizer.normalize(beyt).replace(' ز ', ' از ').replace(' مر ', ' ') #normalize beshe va z beshe az\n",
    "    \n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        past_root = lemmatizer.lemmatize(word).split('#', 1)[0]\n",
    "        past_root = past_root.replace('آ', 'ا')\n",
    "        word_modified = word.replace(past_root, '')\n",
    "        \n",
    "        if word_modified != '':\n",
    "            if word_modified[0] == 'ب' and len(word_modified) < 4:\n",
    "                #yani fele maazi bode be forme ghadimi\n",
    "                beyt = beyt.replace(word, word[1:])\n",
    "        \n",
    "        if word.endswith('گه'):\n",
    "            beyt = beyt.replace(word, word[:-2]+'گاه')\n",
    "        \n",
    "        return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    df.at[i, 'beyt'] = preprocess_1(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2654",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی دوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، حالات باستانی افعال به حالت عادی تبدیل می‌شود تا افعال یک دست باشند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2dfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        \n",
    "        if word[0] == 'م':\n",
    "            word_modified = 'ب' + word[1:]\n",
    "            present_root = lemmatizer.lemmatize(word_modified).split('#', 1)[-1]\n",
    "            present_root = present_root.replace('آ', 'ا')\n",
    "            word_modified = word_modified.replace(present_root, '')\n",
    "            \n",
    "            if word_modified == 'ب':\n",
    "                beyt = beyt.replace(word, 'ن'+word[1:])\n",
    "    return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):            \n",
    "    df.at[i, 'beyt'] = preprocess_2(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c6a20",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی سوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این مرحله، استاپ ووردها حذف شده و برخی واژگان خاص پیش‌پردازش می‌شوند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac84c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Exception.txt', encoding='utf8') as exc:\n",
    "    data = exc.read()\n",
    "special_verbs = json.loads(data)\n",
    "\n",
    "df_temp = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "stopwords = ['!','،','؟','ز','ار']+[normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
    "\n",
    "\n",
    "def preprocess_3(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    words = [t for t in words if t not in stopwords]\n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for t in words:\n",
    "        if t in special_verbs.keys():\n",
    "            lemmatized_tokens.append(t)\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(t).split('#')[0])\n",
    "    \n",
    "    lemmatized = ' '.join(lemmatized_tokens)\n",
    "            \n",
    "    return lemmatized, lemmatized_tokens\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pp = preprocess_3(df.at[i, 'beyt'])\n",
    "            \n",
    "    df.at[i, 'beyt'] = pp[0]\n",
    "    df_temp.at[i, 'beyt'] = pp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f83712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "باب ششم در قناعت\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a253aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'نپنداری قول معقول چو قانع شد سیم سنگ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test preprocessing\n",
    "preprocess_3(preprocess_2(preprocess_1('''نپنداری این قول معقول نیست\n",
    "چو قانع شدی سیم و سنگت یکی است''')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107e49e",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>تابع پیش‌پردازش تک‌بیت</h4>\n",
    "  <br>\n",
    "    حال یک تابع درست می‌کنیم که یک بیت را پیش‌پردازش کند تا بعدا هنگام دریافت ورودی سامانه از آن استفاده کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eba86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(beyt):\n",
    "    return preprocess_3(preprocess_2(preprocess_1(beyt)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0185d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری این قول معقول نیست\n",
      "چو قانع شدی سیم و سنگت یکی است\n",
      "----\n",
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "----\n",
      "نپنداری قول معقول چو قانع شد سیم سنگ\n"
     ]
    }
   ],
   "source": [
    "#test preprocessing functions\n",
    "print(df_raw.at[2000, 'beyt'] + '\\n----')\n",
    "print(df.at[2000, 'beyt'] + '\\n----')\n",
    "print(preprocess(df_raw.at[2000, 'beyt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d212a",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت سوم: کلاس‌بندی با scikit</h3>\n",
    "  <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bff98",
   "metadata": {},
   "source": [
    "<div align=center dir='rtl'>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل اول:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                Logistic Regression (+ Isolation Forest)\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af5353b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for classification (1)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a62801ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d417b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare\n",
    "df = shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "X_text = [x.strip().split() for x in df.beyt]\n",
    "X_vec = vectorizer.fit_transform([' '.join(x) for x in X_text])\n",
    "y = [x for x in df.baab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d588219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3845x4886 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 27936 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57664a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logreg_model(x_in, y_in):\n",
    "    model_1 = LogisticRegression(random_state=0, max_iter=200)\n",
    "    cv_results = cross_validate(model_1, x_in, y_in, cv=5, return_estimator=True, scoring='f1_macro')\n",
    "    print(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820fbc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15926461 0.17260103 0.169607   0.18972463 0.19634209]\n"
     ]
    }
   ],
   "source": [
    "make_logreg_model(X_vec, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0358a3",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>بررسی آوگمنتیشن دیتاست</h4>\n",
    "  <br>\n",
    "    با توجه به دقت کم، در این قسمت بررسی می‌کنیم که افزایش مصنوعی دیتاست چه کمکی ممکن است به ما بکند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7dded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noise = np.random.normal(0, 0.1, X_vec.shape)\n",
    "X_noisy_duplicate = X_noise + X_vec.toarray()\n",
    "X_augmented = np.vstack((X_vec.toarray(), X_noisy_duplicate))\n",
    "y_augmented = y*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b07161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15901636 0.15865    0.19508017 0.17765603 0.20991097]\n"
     ]
    }
   ],
   "source": [
    "make_logreg_model(X_augmented, y_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40dc640",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "  <br>\n",
    "    بنابراین مشاهده می‌شود که با دو برابر کردن دیتاست، بهترین جواب موجود، تا حدی بهتر است و امتیاز f1\n",
    "    از حدود ۰.۱۷۷ برای بهترین مدل، به ۰.۲۰۲ برای بهترین مدل رسید. اما هنوز هم بخاطر نزدیک بودن داده‌ها، این امتیاز چنگی به دل نمی‌زند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925433d",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>تحلیل خروجی‌های یک مدل</h4>\n",
    "  <br>\n",
    "    حال که حدود دقت برای این نوع مدل را می‌دانیم، یک مدل می‌سازیم و متریک‌های خواسته شده و فیچرها را مشاهده می‌کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff03829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2685ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression(random_state=0, max_iter=200).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4cd63f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17705612549388353"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_inf = model_1.predict(X_test)\n",
    "f1_score(y_test, y_inf, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a85d5bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26603119584055457"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52a2124d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[155,   0,  17,  11,   4,  15,   7,  17,   2,  21],\n",
       "       [ 14,   0,   5,   1,   0,   3,   0,   3,   0,   6],\n",
       "       [ 68,   0,  35,  10,   0,   2,   2,  16,   3,  18],\n",
       "       [ 37,   1,  12,  20,   0,   8,   4,   9,   3,  16],\n",
       "       [ 19,   0,  11,   4,   3,   1,   3,   5,   3,   7],\n",
       "       [ 36,   0,  14,   5,   1,  13,   6,   8,   1,  10],\n",
       "       [ 41,   0,   2,   6,   0,   6,   5,   7,   0,  15],\n",
       "       [ 69,   1,  12,   6,   4,   5,   2,  26,   2,  13],\n",
       "       [ 25,   1,   8,   4,   0,   3,   1,   6,   4,   7],\n",
       "       [ 74,   2,  15,   7,   2,  10,   2,  17,   3,  46]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a504d",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>فیچرها</h4>\n",
    "  <br>\n",
    "    مشاهده می‌کنیم که چه فیچرهایی انتخاب شده‌اند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "609fc37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ملک', 'عدو', 'دشمن', 'لشکر', 'ظالم', 'سپاه', 'رعیت', 'جنگ', 'کشور', 'گزند']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vectorizer.get_feature_names()[i] for i in model_1.coef_[0].argsort()[-10:][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a4b417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['آب',\n",
       " 'آباد',\n",
       " 'آبدان',\n",
       " 'آبرو',\n",
       " 'آبستن',\n",
       " 'آبکش',\n",
       " 'آبگینه',\n",
       " 'آتش',\n",
       " 'آتشم',\n",
       " 'آتشین',\n",
       " 'آثار',\n",
       " 'آختن',\n",
       " 'آخته',\n",
       " 'آخر',\n",
       " 'آخرت',\n",
       " 'آداب',\n",
       " 'آدم',\n",
       " 'آدمی',\n",
       " 'آذر',\n",
       " 'آذرپرست',\n",
       " 'آذین',\n",
       " 'آر',\n",
       " 'آرا',\n",
       " 'آراسته',\n",
       " 'آرام',\n",
       " 'آرایش',\n",
       " 'آرد',\n",
       " 'آرزومند',\n",
       " 'آرش',\n",
       " 'آرم',\n",
       " 'آرمید',\n",
       " 'آرند',\n",
       " 'آری',\n",
       " 'آز',\n",
       " 'آزاد',\n",
       " 'آزادمرد',\n",
       " 'آزاده',\n",
       " 'آزادگان',\n",
       " 'آزادگی',\n",
       " 'آزار',\n",
       " 'آزرده',\n",
       " 'آزمای',\n",
       " 'آزمود',\n",
       " 'آزمودم',\n",
       " 'آزموده',\n",
       " 'آزمودی',\n",
       " 'آسان',\n",
       " 'آساید',\n",
       " 'آسایدش',\n",
       " 'آسایش',\n",
       " 'آستان',\n",
       " 'آستر',\n",
       " 'آستین',\n",
       " 'آسمان',\n",
       " 'آسود',\n",
       " 'آسوده',\n",
       " 'آسودگی',\n",
       " 'آسیب',\n",
       " 'آشتی',\n",
       " 'آشفت',\n",
       " 'آشفته',\n",
       " 'آشفتگی',\n",
       " 'آشنا',\n",
       " 'آشنایی',\n",
       " 'آشوب',\n",
       " 'آشکار',\n",
       " 'آغاز',\n",
       " 'آغشته',\n",
       " 'آغوش',\n",
       " 'آفاق',\n",
       " 'آفتاب',\n",
       " 'آفرید',\n",
       " 'آفریدت',\n",
       " 'آفریدندت',\n",
       " 'آفرین',\n",
       " 'آفریند',\n",
       " 'آفریننده',\n",
       " 'آل',\n",
       " 'آلایش',\n",
       " 'آلایشم',\n",
       " 'آلوده',\n",
       " 'آماج',\n",
       " 'آماجگاه',\n",
       " 'آمد',\n",
       " 'آمدت',\n",
       " 'آمدش',\n",
       " 'آمدن',\n",
       " 'آمده',\n",
       " 'آمرزگار',\n",
       " 'آموخت',\n",
       " 'آموخته',\n",
       " 'آموز',\n",
       " 'آموزگار',\n",
       " 'آمیختستند',\n",
       " 'آمیزگار',\n",
       " 'آمیزگاری',\n",
       " 'آن',\n",
       " 'آنچ',\n",
       " 'آنگه',\n",
       " 'آنی',\n",
       " 'آه',\n",
       " 'آهخت',\n",
       " 'آهخته',\n",
       " 'آهسته',\n",
       " 'آهستگی',\n",
       " 'آهن',\n",
       " 'آهنی',\n",
       " 'آهنین',\n",
       " 'آهو',\n",
       " 'آوارگی',\n",
       " 'آواز',\n",
       " 'آوازه',\n",
       " 'آوخ',\n",
       " 'آور',\n",
       " 'آوران',\n",
       " 'آورد',\n",
       " 'آوردت',\n",
       " 'آوردش',\n",
       " 'آوردمی',\n",
       " 'آوردن',\n",
       " 'آورده',\n",
       " 'آورست',\n",
       " 'آوری',\n",
       " 'آوریدند',\n",
       " 'آویخت',\n",
       " 'آویخته',\n",
       " 'آویزدت',\n",
       " 'آکنده',\n",
       " 'آگاه',\n",
       " 'آگاهی',\n",
       " 'آگه',\n",
       " 'آگهی',\n",
       " 'آی',\n",
       " 'آیت',\n",
       " 'آیدت',\n",
       " 'آیدش',\n",
       " 'آیدم',\n",
       " 'آینه',\n",
       " 'آیین',\n",
       " 'آیینه',\n",
       " 'أحسن',\n",
       " 'أساء',\n",
       " 'إلی',\n",
       " 'ابجد',\n",
       " 'ابدال',\n",
       " 'ابر',\n",
       " 'ابره',\n",
       " 'ابرو',\n",
       " 'ابروان',\n",
       " 'ابریشمین',\n",
       " 'ابل',\n",
       " 'ابلاغ',\n",
       " 'ابله',\n",
       " 'ابلیس',\n",
       " 'ابن',\n",
       " 'ابو',\n",
       " 'ابوبکر',\n",
       " 'اتابک',\n",
       " 'اتفاق',\n",
       " 'اجرت',\n",
       " 'اجل',\n",
       " 'احتشام',\n",
       " 'احتقارم',\n",
       " 'احسان',\n",
       " 'احسنت',\n",
       " 'احوال',\n",
       " 'اخبار',\n",
       " 'اختر',\n",
       " 'اختیار',\n",
       " 'اخلاص',\n",
       " 'اخلاق',\n",
       " 'اخگر',\n",
       " 'ادا',\n",
       " 'ادب',\n",
       " 'ادرار',\n",
       " 'ادراک',\n",
       " 'ادهم',\n",
       " 'ادهمی',\n",
       " 'ارادت',\n",
       " 'ارباب',\n",
       " 'اردبیل',\n",
       " 'اردیبهشت',\n",
       " 'ارزان',\n",
       " 'ارسلان',\n",
       " 'ارغوان',\n",
       " 'ارکان',\n",
       " 'از',\n",
       " 'ازرق',\n",
       " 'ازل',\n",
       " 'اساس',\n",
       " 'اسب',\n",
       " 'اسباب',\n",
       " 'استا',\n",
       " 'استاد',\n",
       " 'استاده',\n",
       " 'استخوان',\n",
       " 'استخوانند',\n",
       " 'استعانت',\n",
       " 'استوار',\n",
       " 'اسرار',\n",
       " 'اسفندیار',\n",
       " 'اسلم',\n",
       " 'اسم',\n",
       " 'اسودگی',\n",
       " 'اسیر',\n",
       " 'اشارت',\n",
       " 'اشتر',\n",
       " 'اشک',\n",
       " 'اصحاب',\n",
       " 'اصل',\n",
       " 'اصم',\n",
       " 'اصول',\n",
       " 'اطراف',\n",
       " 'اطفال',\n",
       " 'اطلس',\n",
       " 'اعتبار',\n",
       " 'اعتقاد',\n",
       " 'اعلا',\n",
       " 'اعلی',\n",
       " 'اعمال',\n",
       " 'اغیار',\n",
       " 'افتاد',\n",
       " 'افتاده',\n",
       " 'افتادگان',\n",
       " 'افتادگی',\n",
       " 'افتان',\n",
       " 'افتدش',\n",
       " 'افراختند',\n",
       " 'افرازدت',\n",
       " 'افرازدم',\n",
       " 'افراسیاب',\n",
       " 'افراشتن',\n",
       " 'افراشتی',\n",
       " 'افروخت',\n",
       " 'افروختم',\n",
       " 'افروختن',\n",
       " 'افروخته',\n",
       " 'افزون',\n",
       " 'افسانه',\n",
       " 'افسوس',\n",
       " 'افشاند',\n",
       " 'افطار',\n",
       " 'افلاک',\n",
       " 'افکن',\n",
       " 'افکند',\n",
       " 'افکنده',\n",
       " 'افکنی',\n",
       " 'افگار',\n",
       " 'اقامت',\n",
       " 'اقبال',\n",
       " 'اقبیل',\n",
       " 'اقرار',\n",
       " 'اقصای',\n",
       " 'اقطاع',\n",
       " 'اقلیم',\n",
       " 'الا',\n",
       " 'الب',\n",
       " 'التفات',\n",
       " 'التهاب',\n",
       " 'الحرام',\n",
       " 'الحمد',\n",
       " 'الدهر',\n",
       " 'السبیل',\n",
       " 'الست',\n",
       " 'السلام',\n",
       " 'الصفات',\n",
       " 'الصلوة',\n",
       " 'الطاف',\n",
       " 'العالمین',\n",
       " 'الغیب',\n",
       " 'الف',\n",
       " 'القرین',\n",
       " 'القصه',\n",
       " 'الله',\n",
       " 'اللیل',\n",
       " 'الم',\n",
       " 'المثل',\n",
       " 'المقدس',\n",
       " 'النهار',\n",
       " 'اله',\n",
       " 'الها',\n",
       " 'الوان',\n",
       " 'الوقت',\n",
       " 'الوند',\n",
       " 'الکریم',\n",
       " 'الیم',\n",
       " 'ام',\n",
       " 'اماره',\n",
       " 'امان',\n",
       " 'امانت',\n",
       " 'امر',\n",
       " 'امروز',\n",
       " 'امسال',\n",
       " 'امشب',\n",
       " 'امل',\n",
       " 'املاک',\n",
       " 'امید',\n",
       " 'امیدوار',\n",
       " 'امیر',\n",
       " 'امین',\n",
       " 'ان',\n",
       " 'انائی',\n",
       " 'انبار',\n",
       " 'انباز',\n",
       " 'انبان',\n",
       " 'انبوب',\n",
       " 'انبیا',\n",
       " 'انتظار',\n",
       " 'انجام',\n",
       " 'انجم',\n",
       " 'انجمن',\n",
       " 'انجیل',\n",
       " 'اند',\n",
       " 'انداخت',\n",
       " 'انداخته',\n",
       " 'انداز',\n",
       " 'اندازدت',\n",
       " 'اندازه',\n",
       " 'اندام',\n",
       " 'اندر',\n",
       " 'اندرز',\n",
       " 'اندرست',\n",
       " 'اندرند',\n",
       " 'اندرون',\n",
       " 'اندرونی',\n",
       " 'انده',\n",
       " 'اندوختن',\n",
       " 'اندوختند',\n",
       " 'اندوخته',\n",
       " 'اندوده',\n",
       " 'اندودگان',\n",
       " 'اندوهگین',\n",
       " 'اندک',\n",
       " 'اندیش',\n",
       " 'اندیشناک',\n",
       " 'اندیشه',\n",
       " 'اندیشید',\n",
       " 'انس',\n",
       " 'انسان',\n",
       " 'انصاف',\n",
       " 'انعام',\n",
       " 'انفتح',\n",
       " 'انگار',\n",
       " 'انگاشتم',\n",
       " 'انگبین',\n",
       " 'انگشت',\n",
       " 'انگشتر',\n",
       " 'انگه',\n",
       " 'انگور',\n",
       " 'انگیختم',\n",
       " 'انگیختن',\n",
       " 'انگیز',\n",
       " 'اهرمن',\n",
       " 'اهل',\n",
       " 'او',\n",
       " 'اوباش',\n",
       " 'اوج',\n",
       " 'اوراق',\n",
       " 'اوست',\n",
       " 'اوصاف',\n",
       " 'اوفتاد',\n",
       " 'اوفتادش',\n",
       " 'اوفتد',\n",
       " 'اوفتند',\n",
       " 'اوفتی',\n",
       " 'اول',\n",
       " 'اولوالعزم',\n",
       " 'اوژن',\n",
       " 'اژدهای',\n",
       " 'اکبر',\n",
       " 'اکرام',\n",
       " 'اکل',\n",
       " 'اگرچه',\n",
       " 'ای',\n",
       " 'ایاز',\n",
       " 'ایام',\n",
       " 'ایثار',\n",
       " 'ایدر',\n",
       " 'ایدون',\n",
       " 'ایذا',\n",
       " 'ایران',\n",
       " 'ایزد',\n",
       " 'ایست',\n",
       " 'ایستاد',\n",
       " 'ایستاده',\n",
       " 'ایمان',\n",
       " 'ایمن',\n",
       " 'این',\n",
       " 'اینان',\n",
       " 'اینک',\n",
       " 'ایوان',\n",
       " 'باالحسن',\n",
       " 'باب',\n",
       " 'بابا',\n",
       " 'بابک',\n",
       " 'باج',\n",
       " 'باخت',\n",
       " 'باختر',\n",
       " 'باختن',\n",
       " 'باد',\n",
       " 'بادام',\n",
       " 'بادسنج',\n",
       " 'بادند',\n",
       " 'بادپا',\n",
       " 'بار',\n",
       " 'باران',\n",
       " 'باردار',\n",
       " 'بارش',\n",
       " 'بارند',\n",
       " 'بارکش',\n",
       " 'بارگاه',\n",
       " 'بارگه',\n",
       " 'بارگی',\n",
       " 'بارگیر',\n",
       " 'باری',\n",
       " 'باریدن',\n",
       " 'باریده',\n",
       " 'باریک',\n",
       " 'باز',\n",
       " 'بازآمد',\n",
       " 'بازار',\n",
       " 'بازارگان',\n",
       " 'بازارگانان',\n",
       " 'بازارگانی',\n",
       " 'بازو',\n",
       " 'بازوان',\n",
       " 'بازوانند',\n",
       " 'بازگشت',\n",
       " 'بازگشتی',\n",
       " 'بازگیری',\n",
       " 'بازی',\n",
       " 'بازیچه',\n",
       " 'باش',\n",
       " 'باشدت',\n",
       " 'باشدش',\n",
       " 'باطل',\n",
       " 'باغ',\n",
       " 'باف',\n",
       " 'باقی',\n",
       " 'بال',\n",
       " 'بالا',\n",
       " 'بالش',\n",
       " 'بالغ',\n",
       " 'بالوعه',\n",
       " 'بالین',\n",
       " 'بام',\n",
       " 'بامداد',\n",
       " 'بامدادان',\n",
       " 'بانو',\n",
       " 'بانگ',\n",
       " 'بانی',\n",
       " 'باهم',\n",
       " 'باور',\n",
       " 'باک',\n",
       " 'باید',\n",
       " 'بایدت',\n",
       " 'بایدم',\n",
       " 'بایزید',\n",
       " 'بایسته',\n",
       " 'بباد',\n",
       " 'ببار',\n",
       " 'ببارید',\n",
       " 'بباید',\n",
       " 'ببایدت',\n",
       " 'ببخشای',\n",
       " 'ببخشایدم',\n",
       " 'ببخشایم',\n",
       " 'ببخشد',\n",
       " 'ببخشش',\n",
       " 'ببخشود',\n",
       " 'ببخشید',\n",
       " 'ببر',\n",
       " 'ببرد',\n",
       " 'ببردند',\n",
       " 'ببردی',\n",
       " 'ببریدی',\n",
       " 'ببست',\n",
       " 'ببستم',\n",
       " 'ببندد',\n",
       " 'ببندند',\n",
       " 'ببود',\n",
       " 'ببودم',\n",
       " 'ببودند',\n",
       " 'ببوسی',\n",
       " 'ببوسید',\n",
       " 'ببیند',\n",
       " 'ببینم',\n",
       " 'ببینی',\n",
       " 'بت',\n",
       " 'بتا',\n",
       " 'بتاب',\n",
       " 'بتابد',\n",
       " 'بتاخت',\n",
       " 'بتازید',\n",
       " 'بتافت',\n",
       " 'بتان',\n",
       " 'بتخانه',\n",
       " 'بتدریج',\n",
       " 'بتر',\n",
       " 'بترسی',\n",
       " 'بتک',\n",
       " 'بتکده',\n",
       " 'بجا',\n",
       " 'بجست',\n",
       " 'بجستش',\n",
       " 'بجستند',\n",
       " 'بجوی',\n",
       " 'بجویند',\n",
       " 'بحث',\n",
       " 'بحر',\n",
       " 'بحل',\n",
       " 'بحمدالله',\n",
       " 'بخار',\n",
       " 'بخاست',\n",
       " 'بخایند',\n",
       " 'بخایندش',\n",
       " 'بخت',\n",
       " 'بختان',\n",
       " 'بختم',\n",
       " 'بختی',\n",
       " 'بختیار',\n",
       " 'بخرد',\n",
       " 'بخردی',\n",
       " 'بخسبد',\n",
       " 'بخسبند',\n",
       " 'بخست',\n",
       " 'بخستی',\n",
       " 'بخش',\n",
       " 'بخشای',\n",
       " 'بخشایش',\n",
       " 'بخشش',\n",
       " 'بخشنده',\n",
       " 'بخشندگی',\n",
       " 'بخشید',\n",
       " 'بخفت',\n",
       " 'بخواست',\n",
       " 'بخواندند',\n",
       " 'بخوردم',\n",
       " 'بخوردند',\n",
       " 'بخوردندی',\n",
       " 'بخوشید',\n",
       " 'بخیز',\n",
       " 'بخیزد',\n",
       " 'بخیل',\n",
       " 'بخیه',\n",
       " 'بد',\n",
       " 'بدا',\n",
       " 'بداد',\n",
       " 'بدادی',\n",
       " 'بدادیم',\n",
       " 'بدان',\n",
       " 'بداندیش',\n",
       " 'بدانست',\n",
       " 'بدانستم',\n",
       " 'بدبخت',\n",
       " 'بدخشان',\n",
       " 'بدخو',\n",
       " 'بدر',\n",
       " 'بدرد',\n",
       " 'بدردش',\n",
       " 'بدرند',\n",
       " 'بدزدید',\n",
       " 'بدزهره',\n",
       " 'بدست',\n",
       " 'بدسگال',\n",
       " 'بدعهد',\n",
       " 'بدلگام',\n",
       " 'بدمرد',\n",
       " 'بدن',\n",
       " 'بدنام',\n",
       " 'بدوخت',\n",
       " 'بدگمان',\n",
       " 'بدگهر',\n",
       " 'بدگو',\n",
       " 'بدی',\n",
       " 'بدید',\n",
       " 'بدیدم',\n",
       " 'بدیدند',\n",
       " 'بدیع',\n",
       " 'بدین',\n",
       " 'بذل',\n",
       " 'بر',\n",
       " 'برآر',\n",
       " 'برآرد',\n",
       " 'برآرم',\n",
       " 'برآرند',\n",
       " 'برآریم',\n",
       " 'برآشفت',\n",
       " 'برآمد',\n",
       " 'برآمیخته',\n",
       " 'برآنند',\n",
       " 'برآهخت',\n",
       " 'برآور',\n",
       " 'برآورد',\n",
       " 'برآورده',\n",
       " 'برآید',\n",
       " 'برآیی',\n",
       " 'برادر',\n",
       " 'بران',\n",
       " 'براند',\n",
       " 'برانداخت',\n",
       " 'برانداختم',\n",
       " 'براندازد',\n",
       " 'براندایدش',\n",
       " 'براندش',\n",
       " 'براندن',\n",
       " 'براندند',\n",
       " 'براندندی',\n",
       " 'براندیش',\n",
       " 'برانگیختیم',\n",
       " 'برانگیز',\n",
       " 'بربست',\n",
       " 'بربط',\n",
       " 'برتافت',\n",
       " 'برتافتن',\n",
       " 'برتافتی',\n",
       " 'برتر',\n",
       " 'برتنند',\n",
       " 'برج',\n",
       " 'برجاس',\n",
       " 'برجست',\n",
       " 'برخاست',\n",
       " 'برخاسته',\n",
       " 'برخاستی',\n",
       " 'برخداست',\n",
       " 'برخور',\n",
       " 'برخوری',\n",
       " 'برخیزد',\n",
       " 'برخیش',\n",
       " 'برد',\n",
       " 'بردار',\n",
       " 'برداشت',\n",
       " 'برداشتم',\n",
       " 'برداشتن',\n",
       " 'برداشتند',\n",
       " 'برداشتی',\n",
       " 'بردبار',\n",
       " 'بردمت',\n",
       " 'بردمی',\n",
       " 'بردن',\n",
       " 'برده',\n",
       " 'بردوختی',\n",
       " 'برست',\n",
       " 'برش',\n",
       " 'برشدی',\n",
       " 'برشکفت',\n",
       " 'برف',\n",
       " 'برفاب',\n",
       " 'برفتش',\n",
       " 'برفتند',\n",
       " 'برفتی',\n",
       " 'برفتیم',\n",
       " 'برفرازنده',\n",
       " 'برفروخت',\n",
       " 'برفروز',\n",
       " 'برفروزد',\n",
       " 'برفشاند',\n",
       " 'برفشاندی',\n",
       " 'برق',\n",
       " 'برقرار',\n",
       " 'برمپیچ',\n",
       " 'برنا',\n",
       " 'برنجاندت',\n",
       " 'برنجد',\n",
       " 'برنجید',\n",
       " 'برنخاست',\n",
       " 'برنداشت',\n",
       " 'برندت',\n",
       " 'برندم',\n",
       " 'برنده',\n",
       " 'برنهاد',\n",
       " 'برنهند',\n",
       " 'برنپیچد',\n",
       " 'برنکرد',\n",
       " 'برنیاید',\n",
       " 'برنیایی',\n",
       " 'بره',\n",
       " 'برهمن',\n",
       " 'برهنه',\n",
       " 'برو',\n",
       " 'بروزید',\n",
       " 'برومند',\n",
       " 'برون',\n",
       " 'برپری',\n",
       " 'برکشند',\n",
       " 'برکشید',\n",
       " 'برکند',\n",
       " 'برکندش',\n",
       " 'برکنده',\n",
       " 'برکنی',\n",
       " 'برگ',\n",
       " 'برگذشت',\n",
       " 'برگرد',\n",
       " 'برگرفت',\n",
       " 'برگشتن',\n",
       " 'برگشته',\n",
       " 'برگشود',\n",
       " 'بری',\n",
       " 'بریخت',\n",
       " 'بریدن',\n",
       " 'بریدند',\n",
       " 'بریزاندت',\n",
       " 'برین',\n",
       " 'بزاد',\n",
       " 'بزارید',\n",
       " 'بزد',\n",
       " 'بزرگ',\n",
       " 'بزشتی',\n",
       " 'بزم',\n",
       " 'بزیست',\n",
       " 'بس',\n",
       " 'بسا',\n",
       " 'بساخت',\n",
       " 'بساط',\n",
       " 'بسامان',\n",
       " 'بست',\n",
       " 'بستان',\n",
       " 'بستاندم',\n",
       " 'بستر',\n",
       " 'بستش',\n",
       " 'بستن',\n",
       " 'بسته',\n",
       " 'بستگان',\n",
       " 'بسرشت',\n",
       " 'بسفت',\n",
       " 'بسند',\n",
       " 'بسنده',\n",
       " 'بسوخت',\n",
       " 'بسوزاندم',\n",
       " 'بسیاردان',\n",
       " 'بسیج',\n",
       " 'بسیچ',\n",
       " 'بشارت',\n",
       " 'بشتافتند',\n",
       " 'بشد',\n",
       " 'بشر',\n",
       " 'بشست',\n",
       " 'بشستند',\n",
       " 'بشستیم',\n",
       " 'بشناختند',\n",
       " 'بشناختی',\n",
       " 'بشنید',\n",
       " 'بشوخی',\n",
       " 'بشورید',\n",
       " 'بشویند',\n",
       " 'بشویی',\n",
       " 'بشکافتیم',\n",
       " 'بشکست',\n",
       " 'بشیر',\n",
       " 'بصر',\n",
       " 'بصره',\n",
       " 'بصیرت',\n",
       " 'بضاعات',\n",
       " 'بضاعت',\n",
       " 'بط',\n",
       " 'بغداد',\n",
       " 'بغرید',\n",
       " 'بغل',\n",
       " 'بغلطاندم',\n",
       " 'بغلطید',\n",
       " 'بفرسود',\n",
       " 'بفرسودم',\n",
       " 'بفرمای',\n",
       " 'بفرمود',\n",
       " 'بفروختندش',\n",
       " 'بفور',\n",
       " 'بفکنندت',\n",
       " 'بفکنی',\n",
       " 'بقا',\n",
       " 'بقال',\n",
       " 'بقراط',\n",
       " 'بقعه',\n",
       " 'بقیمت',\n",
       " 'بلا',\n",
       " 'بلارک',\n",
       " 'بلاست',\n",
       " 'بلاغات',\n",
       " 'بلاغت',\n",
       " 'بلبل',\n",
       " 'بلرزد',\n",
       " 'بلرزیدی',\n",
       " 'بلقیس',\n",
       " 'بلند',\n",
       " 'بلورین',\n",
       " 'بلی',\n",
       " 'بلیغ',\n",
       " 'بلیلانه',\n",
       " 'بم',\n",
       " 'بمال',\n",
       " 'بمالش',\n",
       " 'بمالید',\n",
       " 'بمالیدی',\n",
       " 'بماند',\n",
       " 'بماندش',\n",
       " 'بماندند',\n",
       " 'بماندی',\n",
       " 'بمرد',\n",
       " 'بمردند',\n",
       " 'بن',\n",
       " 'بنا',\n",
       " 'بنازند',\n",
       " 'بنالد',\n",
       " 'بنالم',\n",
       " 'بنالید',\n",
       " 'بنالیدم',\n",
       " 'بناگوش',\n",
       " 'بند',\n",
       " 'بنده',\n",
       " 'بندگان',\n",
       " 'بندگی',\n",
       " 'بنشاند',\n",
       " 'بنشست',\n",
       " 'بنمود',\n",
       " 'بنه',\n",
       " 'بنهاد',\n",
       " 'بنواختش',\n",
       " 'بنگاه',\n",
       " 'بنگریست',\n",
       " 'بنیاد',\n",
       " 'به',\n",
       " 'بهار',\n",
       " 'بهاران',\n",
       " 'بهای',\n",
       " 'بهایم',\n",
       " 'بهتان',\n",
       " 'بهتر',\n",
       " 'بهر',\n",
       " 'بهرام',\n",
       " 'بهره',\n",
       " 'بهشت',\n",
       " 'بهشتی',\n",
       " 'بهل',\n",
       " 'بهلول',\n",
       " 'بهمن',\n",
       " 'بهی',\n",
       " 'بو',\n",
       " 'بوالعجب',\n",
       " 'بوالهوس',\n",
       " 'بوبکر',\n",
       " 'بود',\n",
       " 'بودت',\n",
       " 'بودش',\n",
       " 'بوده',\n",
       " 'بوریا',\n",
       " 'بوس',\n",
       " 'بوستان',\n",
       " 'بوستانبان',\n",
       " 'بوسه',\n",
       " 'بوسید',\n",
       " 'بول',\n",
       " 'بوم',\n",
       " 'بپای',\n",
       " 'بپختم',\n",
       " 'بپرداختن',\n",
       " 'بپرسم',\n",
       " 'بپرسیدش',\n",
       " 'بپرسیدم',\n",
       " 'بپرهیز',\n",
       " 'بپرور',\n",
       " 'بپوشانش',\n",
       " 'بچم',\n",
       " 'بچه',\n",
       " 'بک',\n",
       " 'بکاوند',\n",
       " 'بکرد',\n",
       " 'بکردند',\n",
       " 'بکش',\n",
       " 'بکشتش',\n",
       " 'بکشتی',\n",
       " 'بکندند',\n",
       " 'بکندی',\n",
       " 'بکوب',\n",
       " 'بکوبی',\n",
       " 'بگاه',\n",
       " 'بگذارمش',\n",
       " 'بگذاشتن',\n",
       " 'بگذاشتند',\n",
       " 'بگذرد',\n",
       " 'بگذرند',\n",
       " 'بگذشت',\n",
       " 'بگردان',\n",
       " 'بگرداندت',\n",
       " 'بگرداندش',\n",
       " 'بگردیدش',\n",
       " 'بگردیدمی',\n",
       " 'بگریختم',\n",
       " 'بگریخته',\n",
       " 'بگرید',\n",
       " 'بگریزی',\n",
       " 'بگریست',\n",
       " 'بگریستی',\n",
       " 'بگریند',\n",
       " 'بگسترد',\n",
       " 'بگسست',\n",
       " 'بگسل',\n",
       " 'بگسلاند',\n",
       " 'بگسلاندش',\n",
       " 'بگسلی',\n",
       " 'بگسیخته',\n",
       " 'بگشتی',\n",
       " 'بگشوده',\n",
       " 'بگفت',\n",
       " 'بگفتا',\n",
       " 'بگفتش',\n",
       " 'بگفتم',\n",
       " 'بگفتند',\n",
       " 'بگفتندش',\n",
       " 'بگفتیم',\n",
       " 'بگوی',\n",
       " 'بگویم',\n",
       " 'بی',\n",
       " 'بیا',\n",
       " 'بیابان',\n",
       " 'بیار',\n",
       " 'بیاراست',\n",
       " 'بیاراستم',\n",
       " 'بیاراستند',\n",
       " 'بیاراستی',\n",
       " 'بیارند',\n",
       " 'بیازار',\n",
       " 'بیازرد',\n",
       " 'بیازردمی',\n",
       " 'بیاشوفتی',\n",
       " 'بیافتاد',\n",
       " 'بیافتادن',\n",
       " 'بیافزود',\n",
       " 'بیاموز',\n",
       " 'بیان',\n",
       " 'بیانداز',\n",
       " 'بیت',\n",
       " 'بیتم',\n",
       " 'بیخ',\n",
       " 'بیخته',\n",
       " 'بیخند',\n",
       " 'بید',\n",
       " 'بیداد',\n",
       " 'بیدادگر',\n",
       " 'بیدار',\n",
       " 'بیدق',\n",
       " 'بیدقی',\n",
       " 'بیست',\n",
       " 'بیستم',\n",
       " 'بیش',\n",
       " 'بیشم',\n",
       " 'بیشه',\n",
       " 'بیضه',\n",
       " 'بیغوله',\n",
       " 'بیفتاد',\n",
       " 'بیفتاده',\n",
       " 'بیفتد',\n",
       " 'بیفتی',\n",
       " 'بیفزای',\n",
       " 'بیفزود',\n",
       " 'بیفشان',\n",
       " 'بیفشاند',\n",
       " 'بیفشاندمی',\n",
       " 'بیفشانده',\n",
       " 'بیفشانش',\n",
       " 'بیفکن',\n",
       " 'بیفکند',\n",
       " 'بیم',\n",
       " 'بیمار',\n",
       " 'بین',\n",
       " 'بینا',\n",
       " 'بیناست',\n",
       " 'بینان',\n",
       " 'بینایی',\n",
       " 'بینداخت',\n",
       " 'بینداختم',\n",
       " 'بینداختند',\n",
       " 'بینداز',\n",
       " 'بیندیش',\n",
       " 'بینش',\n",
       " 'بینمت',\n",
       " 'بینمش',\n",
       " 'بیننده',\n",
       " 'بینندگان',\n",
       " 'بینوا',\n",
       " 'بینی',\n",
       " 'بیهوده',\n",
       " 'بیور',\n",
       " 'بیوه',\n",
       " 'بیچاره',\n",
       " 'بیچارگان',\n",
       " 'بیچارگی',\n",
       " 'بیژن',\n",
       " 'بیگانه',\n",
       " 'بیگانگان',\n",
       " 'بیگانگی',\n",
       " 'تأدیب',\n",
       " 'تأمل',\n",
       " 'تأویل',\n",
       " 'تأیید',\n",
       " 'تاب',\n",
       " 'تابان',\n",
       " 'تابوت',\n",
       " 'تاج',\n",
       " 'تاجدار',\n",
       " 'تاجور',\n",
       " 'تاجگاه',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808fb649",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>بررسی outlier detection</h4>\n",
    "  <br>\n",
    "    سعی می‌کنیم با استفاده از isolation forest یک سیستم\n",
    "    برای تشخیص اشعاری که مرتبط با سعدی نیستند داشته باشیم. \n",
    "    کاربرد این لایه برای زمانی است که بخواهیم یک بیت دلخواه را کلاس‌بندی کنیم که ممکن است از بوستان نباشد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99888531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isfo = IsolationForest(random_state=0).fit(X_train)\n",
    "isfo.predict(X_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a4f40ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1, -1, -1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_input = X_test[0:5] + np.random.normal(0, 0.1, X_test[0:5].shape)\n",
    "isfo.predict(noisy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35a5818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_input = np.random.normal(0, 0.5, X_test[0:5].shape)\n",
    "isfo.predict(random_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96018d94",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "  <br>\n",
    "    بنابراین مشاهده می‌شود که عملکرد این مدل روی داده‌هایی که از بوستان داریم مناسب است و لااقل فرق بین ابیات نویزی و ابیات عادی تا حد خوبی مشخص است و فرق بیت داده‌ی رندوم و ابیات عادی خوب است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c257641",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل دوم:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                مدل مبتنی بر ترنسفرمر\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdc8f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForSequenceClassification, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "#model: https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e7b8650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d6cdb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "labels_dict = {}\n",
    "labels_dict['باب اول در عدل و تدبیر و رای'] = 0\n",
    "labels_dict['باب دوم در احسان'] = 1\n",
    "labels_dict['باب سوم در عشق و مستی و شور'] = 2\n",
    "labels_dict['باب چهارم در تواضع'] = 3\n",
    "labels_dict['باب پنجم در رضا'] = 4\n",
    "labels_dict['باب ششم در قناعت'] = 5\n",
    "labels_dict['باب هفتم در عالم تربیت'] = 6\n",
    "labels_dict['باب هشتم در شکر بر عافیت'] = 7\n",
    "labels_dict['باب نهم در توبه و راه صواب'] = 8\n",
    "labels_dict['باب دهم در مناجات و ختم کتاب'] = 9\n",
    "for i in range(0, df.shape[0]):\n",
    "    X.append(df.at[i, 'beyt'])\n",
    "    Y.append(labels_dict[df.at[i, 'baab'].strip()])\n",
    "X_train, X_testdev, Y_train, Y_testdev = train_test_split(X, Y, test_size=0.2, random_state=1, shuffle=True)\n",
    "X_test, X_dev, Y_test, Y_dev = train_test_split(X_testdev, Y_testdev, test_size=0.5, random_state=1, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfa815eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
    "dev_encodings = tokenizer(X_dev, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9dec2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = PoemDataset(train_encodings, Y_train)\n",
    "test_dataset = PoemDataset(test_encodings, Y_test)\n",
    "dev_dataset = PoemDataset(dev_encodings, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8d96da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sky\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3076\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 100\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 310\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 45:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.904100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.333500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.041600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=310, training_loss=1.82855468257781, metrics={'train_runtime': 2724.4047, 'train_samples_per_second': 11.291, 'train_steps_per_second': 0.114, 'total_flos': 268742022860640.0, 'train_loss': 1.82855468257781, 'epoch': 10.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=100,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf68cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17683485360544685"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-classification', model, tokenizer=tokenizer, config={'max_length':256})\n",
    "Y_pred = [int(y['label'].split('_')[1]) for y in generator(X_test)]\n",
    "f1_score(Y_test, Y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f105d3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2786458333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd06faeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[155,   0,  17,  11,   4,  15,   7,  17,   2,  21],\n",
       "       [ 14,   0,   5,   1,   0,   3,   0,   3,   0,   6],\n",
       "       [ 68,   0,  35,  10,   0,   2,   2,  16,   3,  18],\n",
       "       [ 37,   1,  12,  20,   0,   8,   4,   9,   3,  16],\n",
       "       [ 19,   0,  11,   4,   3,   1,   3,   5,   3,   7],\n",
       "       [ 36,   0,  14,   5,   1,  13,   6,   8,   1,  10],\n",
       "       [ 41,   0,   2,   6,   0,   6,   5,   7,   0,  15],\n",
       "       [ 69,   1,  12,   6,   4,   5,   2,  26,   2,  13],\n",
       "       [ 25,   1,   8,   4,   0,   3,   1,   6,   4,   7],\n",
       "       [ 74,   2,  15,   7,   2,  10,   2,  17,   3,  46]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4155a",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل سوم:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                مدل شبکه عصبی استفاده از کتابخانه\n",
    "                \n",
    "torchtext\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8f1fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for classification (1)\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ef2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "corresponding_label = {\"باب اول در عدل و تدبیر و رای\":1,\"باب دوم در احسان\":2,\"باب سوم در عشق و مستی و شور\":3,\"باب چهارم در تواضع\":4,\"باب پنجم در رضا\":5,\"باب ششم در قناعت\":6,\"باب هفتم در عالم تربیت\":7,\"باب هشتم در شکر بر عافیت\":8,\"باب نهم در توبه و راه صواب\":9,'باب دهم در مناجات و ختم کتاب ':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3cb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, norm='l2')\n",
    "v = vectorizer.fit_transform(['نپنداری قول معقول چو قانع شد سیم سنگ', 'ممد ممد', 'ممد'])\n",
    "#v = augment(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "281209c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data to training data and validation \n",
    "X_train, X_valid, Y_train, Y_valid= train_test_split(df['beyt'].tolist(),\\\n",
    "                                                      df['baab'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = df['baab'].tolist(),\\\n",
    "                                                      random_state=3)\n",
    "\n",
    "train_dat =list(zip(Y_train,X_train))\n",
    "valid_dat =list(zip(Y_valid,X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6eb9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the voceb\n",
    "train_iter = train_dat\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield word_tokenize(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "901c5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define label and data piplines\n",
    "text_pipeline = lambda x: vocab(word_tokenize(x))\n",
    "label_pipeline = lambda x: corresponding_label[x]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51a26514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one layer model\n",
    "class TextClassificationModel1(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel1, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "#two-layer model\n",
    "class TextClassificationModel2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel2, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,16)\n",
    "        self.fc2 = nn.Linear(16,num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#three-layer model\n",
    "class TextClassificationModel3(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel3, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,16)\n",
    "        self.fc2 = nn.Linear(16,num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "811ed9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40700bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding size is a hyperparameter which we should tune it\n",
    "emsize = 32\n",
    "train_iter1 = train_dat\n",
    "num_class = len(set([label for (label, text) in train_iter1]))\n",
    "vocab_size = len(vocab)\n",
    "model = TextClassificationModel1(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e890d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the data\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            # print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "            #       '| accuracy {:8.3f}| loss {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "            #                                   total_acc/total_count, loss.item()))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "#evaluate the validation data\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            y_pred.append(predicted_label.argmax(1).tolist())\n",
    "            y_true.append(label.tolist())\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    print(metrics.confusion_matrix(list(itertools.chain(*y_true)), list(itertools.chain(* y_pred)), labels=[0,1,2,3,4,5,6,7,8,9]))\n",
    "    return total_acc/total_count, f1_score(list(itertools.chain(*y_true)),list(itertools.chain(* y_pred)), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dac1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 20 # epoch\n",
    "LR = 10 # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecfafa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter2 = train_dat\n",
    "valid_iter2= valid_dat\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66c557eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89 26  8 25  2  1 17 10  4  0]\n",
      " [22 35  2 21  1  1  6  7  4  1]\n",
      " [18 14 13 15  0  0  8  0  5  0]\n",
      " [23 19  6 32  0  3 17  1  4  0]\n",
      " [11  7  4  7  0  1  6  1  3  1]\n",
      " [10  7  0  8  0  3  3  2  0  1]\n",
      " [29 13  4 11  0  3 21  3  5  0]\n",
      " [11 11  2 17  2  0  7  3  2  0]\n",
      " [18  9  2 15  0  1  5  2 15  0]\n",
      " [ 4  2  0  8  0  1  5  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.18s | valid accuracy    0.274 | f1_macro:    0.188  \n",
      "-----------------------------------------------------------\n",
      "[[107  22   8  22   2   1  13   3   4   0]\n",
      " [ 30  29   3  23   0   1   7   3   3   1]\n",
      " [ 21  13  16  15   0   0   6   0   2   0]\n",
      " [ 29  12   6  36   0   3  15   1   3   0]\n",
      " [ 12   6   5   8   0   1   4   1   3   1]\n",
      " [ 10   6   0   8   0   4   3   0   1   2]\n",
      " [ 32  11   4  13   0   3  19   3   4   0]\n",
      " [ 18  11   1  16   2   0   4   2   1   0]\n",
      " [ 22   7   2  13   0   1   5   2  15   0]\n",
      " [  7   2   0   7   0   1   4   1   1   0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.17s | valid accuracy    0.296 | f1_macro:    0.199  \n",
      "-----------------------------------------------------------\n",
      "[[89 18 13 27  5  3  8 12  7  0]\n",
      " [19 23  9 26  1  2  3  9  8  0]\n",
      " [15  5 24 16  0  1  6  0  6  0]\n",
      " [23 10 10 36  2  5 13  2  4  0]\n",
      " [11  6  6  7  0  2  4  1  3  1]\n",
      " [ 8  6  0  7  0  5  3  3  1  1]\n",
      " [28 10  4 15  0  4 19  4  5  0]\n",
      " [15  5  3 17  3  0  4  4  4  0]\n",
      " [20  5  3 17  1  1  2  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.18s | valid accuracy    0.280 | f1_macro:    0.201  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [ 9  6  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.17s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.19s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.19s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.19s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.17s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.17s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.17s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  18 | time:  0.18s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  19 | time:  0.17s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n",
      "[[89 22 11 28  3  3  9 11  6  0]\n",
      " [18 27  5 27  1  1  6  7  7  1]\n",
      " [14  7 22 16  0  1  6  0  7  0]\n",
      " [21 10  8 38  2  6 15  2  3  0]\n",
      " [10  5  5  8  0  2  6  1  3  1]\n",
      " [ 8  6  0  7  0  6  3  2  1  1]\n",
      " [28 11  4 16  0  3 19  3  5  0]\n",
      " [14  6  2 17  3  0  6  3  4  0]\n",
      " [20  5  2 17  1  1  3  3 15  0]\n",
      " [ 4  2  0  9  0  1  4  2  1  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  20 | time:  0.19s | valid accuracy    0.285 | f1_macro:    0.206  \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val,f_macro = evaluate(valid_dataloader)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} | f1_macro: {:8.3f}  '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val,f_macro))\n",
    "    print('-' * 59)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14958d3aee5f1cad06795f787e54b96185c25fb40dfec723a5be941f3a531b8c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
