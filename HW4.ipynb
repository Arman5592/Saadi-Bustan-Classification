{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7346b726",
   "metadata": {},
   "source": [
    "<div align=\"center\" dir=\"rtl\">\n",
    "<h2>تمرین چهارم درس بازیابی پیشرفته‌ی اطلاعات</h2>\n",
    "<h3>گروه سی‌و‌سوم: اسامی</h3>\n",
    "</div>\n",
    "<div align=\"right\" dir=\"rtl\">\n",
    "  <br>\n",
    "    در این تمرین، تسک دسته‌بندی روی داده‌های گرد‌آوری شده از بوستان سعدی انجام می‌شود. ابتدا دیتاست مورد نیازمان از روی داده‌های خام که برای تمرین سوم جمع شده بود ساخته می‌شود، سپس قسمت پیش‌پردازش قرار دارد و در ادامه تسک روی داده‌ها انجام می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1bf1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت اول: آماده‌سازی دیتاست</h3>\n",
    "  <br>\n",
    "    در این قسمت فایل‌های داده‌های خام خوانده شده و دیتاست بیت به بیت برای کلاس‌بندی ایجاد می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c48ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk==3.3->hazm) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Using cached torchtext-0.12.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (1.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (4.62.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==1.11.0->torchtext) (4.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#import libraries for dataset generation\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "!pip install hazm\n",
    "!pip install torchtext\n",
    "from hazm import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2eabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = listdir('Boostan/')\n",
    "\n",
    "i = 0\n",
    "df = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "\n",
    "for p in all_files:\n",
    "    f = open('Boostan/' + p, 'r', encoding='utf8')\n",
    "    \n",
    "    has_poems = True\n",
    "    \n",
    "    while has_poems:\n",
    "        poem = ''\n",
    "        \n",
    "        while True:\n",
    "            s = f.readline()\n",
    "            if '$' in s:\n",
    "                has_poems = False\n",
    "                break\n",
    "            elif '_' in s:\n",
    "                break\n",
    "            \n",
    "            #age injaiim yani s ye mesra boode\n",
    "            beyt = s + f.readline()[:-1]\n",
    "            df.loc[i] = [beyt, p[:-4]]\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3783c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "باب ششم در قناعت\n",
      "3845\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178f294",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت دوم: پیش‌پردازش</h3>\n",
    "  <br>\n",
    "    در این قسمت یک نمونه‌ی پیش‌پردازش شده از دیتاست تهیه می‌شود. دیتاست پیش‌پردازش‌نشده هم در کنار آن حفظ می‌شود که امکان مقایسه‌ی نتیجه روی دو دیتاست وجود داشته باشد.\n",
    "    توضیحات دقیق این که هر گام از پیش‌پردازش شامل چه مواردی است، در تمرین سوم آورده شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0013c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for preprocessing\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eeef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac674b8",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی اول پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، برخی موارد در واژگان (مانند «گه» که همان «گاه» است) اصلاح می‌شوند. عملیات نرمالایز کردن و لمتایز کردن هم انجام داده می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e4ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def preprocess_1(beyt):\n",
    "    beyt = normalizer.normalize(beyt).replace(' ز ', ' از ').replace(' مر ', ' ') #normalize beshe va z beshe az\n",
    "    \n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        past_root = lemmatizer.lemmatize(word).split('#', 1)[0]\n",
    "        past_root = past_root.replace('آ', 'ا')\n",
    "        word_modified = word.replace(past_root, '')\n",
    "        \n",
    "        if word_modified != '':\n",
    "            if word_modified[0] == 'ب' and len(word_modified) < 4:\n",
    "                #yani fele maazi bode be forme ghadimi\n",
    "                beyt = beyt.replace(word, word[1:])\n",
    "        \n",
    "        if word.endswith('گه'):\n",
    "            beyt = beyt.replace(word, word[:-2]+'گاه')\n",
    "        \n",
    "        return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    df.at[i, 'beyt'] = preprocess_1(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2654",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی دوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، حالات باستانی افعال به حالت عادی تبدیل می‌شود تا افعال یک دست باشند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d2dfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        \n",
    "        if word[0] == 'م':\n",
    "            word_modified = 'ب' + word[1:]\n",
    "            present_root = lemmatizer.lemmatize(word_modified).split('#', 1)[-1]\n",
    "            present_root = present_root.replace('آ', 'ا')\n",
    "            word_modified = word_modified.replace(present_root, '')\n",
    "            \n",
    "            if word_modified == 'ب':\n",
    "                beyt = beyt.replace(word, 'ن'+word[1:])\n",
    "    return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):            \n",
    "    df.at[i, 'beyt'] = preprocess_2(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c6a20",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی سوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این مرحله، استاپ ووردها حذف شده و برخی واژگان خاص پیش‌پردازش می‌شوند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac84c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Exception.txt', encoding='utf8') as exc:\n",
    "    data = exc.read()\n",
    "special_verbs = json.loads(data)\n",
    "\n",
    "df_temp = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "stopwords = ['!','،','؟','ز','ار']+[normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
    "\n",
    "\n",
    "def preprocess_3(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    words = [t for t in words if t not in stopwords]\n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for t in words:\n",
    "        if t in special_verbs.keys():\n",
    "            lemmatized_tokens.append(t)\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(t).split('#')[0])\n",
    "    \n",
    "    lemmatized = ' '.join(lemmatized_tokens)\n",
    "            \n",
    "    return lemmatized, lemmatized_tokens\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pp = preprocess_3(df.at[i, 'beyt'])\n",
    "            \n",
    "    df.at[i, 'beyt'] = pp[0]\n",
    "    df_temp.at[i, 'beyt'] = pp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f83712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع سیم سنگ\n",
      "باب ششم در قناعت\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1a253aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'نپنداری قول معقول چو قانع شد سیم سنگ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test preprocessing\n",
    "preprocess_3(preprocess_2(preprocess_1('''نپنداری این قول معقول نیست\n",
    "چو قانع شدی سیم و سنگت یکی است''')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107e49e",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>تابع پیش‌پردازش تک‌بیت</h4>\n",
    "  <br>\n",
    "    حال یک تابع درست می‌کنیم که یک بیت را پیش‌پردازش کند تا بعدا هنگام دریافت ورودی سامانه از آن استفاده کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eba86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(beyt):\n",
    "    return preprocess_3(preprocess_2(preprocess_1(beyt)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0185d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "----\n",
      "نپنداری قول معقول چو قانع سیم سنگ\n",
      "----\n",
      "نپنداری قول معقول چو قانع سیم سنگ\n"
     ]
    }
   ],
   "source": [
    "#test preprocessing functions\n",
    "print(df_raw.at[2000, 'beyt'] + '\\n----')\n",
    "print(df.at[2000, 'beyt'] + '\\n----')\n",
    "print(preprocess(df_raw.at[2000, 'beyt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d212a",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت سوم: کلاس‌بندی با scikit</h3>\n",
    "  <br>\n",
    "    در این قسمت با ؟؟؟ کلاس‌بندی می‌کنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8f1fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for classification (1)\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95ef2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "corresponding_label = {\"باب اول در عدل و تدبیر و رای\":1,\"باب دوم در احسان\":2,\"باب سوم در عشق و مستی و شور\":3,\"باب چهارم در تواضع\":4,\"باب پنجم در رضا\":5,\"باب ششم در قناعت\":6,\"باب هفتم در عالم تربیت\":7,\"باب هشتم در شکر بر عافیت\":8,\"باب نهم در توبه و راه صواب\":9,'باب دهم در مناجات و ختم کتاب ':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd3cb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, norm='l2')\n",
    "v = vectorizer.fit_transform(['نپنداری قول معقول چو قانع شد سیم سنگ', 'ممد ممد', 'ممد'])\n",
    "#v = augment(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bff98",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل اول:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                {اسم مدلتون رو اینجا بنویسید}\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c257641",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل دوم:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                {اسم مدلتون رو اینجا بنویسید}\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4155a",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل سوم:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                مدل شبکه عصبی استفاده از کتابخانه\n",
    "                \n",
    "torchtext\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "281209c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data to training data and validation \n",
    "X_train, X_valid, Y_train, Y_valid= train_test_split(df['beyt'].tolist(),\\\n",
    "                                                      df['baab'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = df['baab'].tolist(),\\\n",
    "                                                      random_state=3)\n",
    "\n",
    "train_dat =list(zip(Y_train,X_train))\n",
    "valid_dat =list(zip(Y_valid,X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6eb9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the voceb\n",
    "train_iter = train_dat\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield word_tokenize(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "901c5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define label and data piplines\n",
    "text_pipeline = lambda x: vocab(word_tokenize(x))\n",
    "label_pipeline = lambda x: corresponding_label[x]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51a26514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one layer model\n",
    "class TextClassificationModel1(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel1, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "#two-layer model\n",
    "class TextClassificationModel2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel2, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,16)\n",
    "        self.fc2 = nn.Linear(16,num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#three-layer model\n",
    "class TextClassificationModel3(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,16)\n",
    "        self.fc2 = nn.Linear(16,num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "811ed9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40700bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding size is a hyperparameter which we should tune it\n",
    "emsize = 32\n",
    "train_iter1 = train_dat\n",
    "num_class = len(set([label for (label, text) in train_iter1]))\n",
    "vocab_size = len(vocab)\n",
    "model = TextClassificationModel1(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e890d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the data\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            # print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "            #       '| accuracy {:8.3f}| loss {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "            #                                   total_acc/total_count, loss.item()))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "#evaluate the validation data\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            y_pred.append(predicted_label.argmax(1).tolist())\n",
    "            y_true.append(label.tolist())\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    print(metrics.confusion_matrix(list(itertools.chain(*y_true)), list(itertools.chain(* y_pred)), labels=[0,1,2,3,4,5,6,7,8,9]))\n",
    "    return total_acc/total_count, f1_score(list(itertools.chain(*y_true)),list(itertools.chain(* y_pred)), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4dac1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 5 # epoch\n",
    "LR = 10 # learning rate\n",
    "BATCH_SIZE = 32 # batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecfafa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter2 = train_dat\n",
    "valid_iter2= valid_dat\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66c557eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[146   5   2  25   0   0   1   0   3   0]\n",
      " [ 69   8   0  20   0   0   0   1   2   0]\n",
      " [ 53   1   1  16   0   0   2   0   0   0]\n",
      " [ 77   7   0  15   0   0   4   0   2   0]\n",
      " [ 32   2   0   7   0   0   0   0   0   0]\n",
      " [ 25   3   0   6   0   0   0   0   0   0]\n",
      " [ 59   1   1  23   0   0   3   0   2   0]\n",
      " [ 34   4   0  15   0   0   1   0   1   0]\n",
      " [ 46   0   1  14   0   0   0   0   6   0]\n",
      " [ 19   0   0   4   0   0   0   0   0   0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.48s | valid accuracy    0.233 | f1_macro:    0.087  \n",
      "-----------------------------------------------------------\n",
      "[[106  17   3  50   0   0   1   1   4   0]\n",
      " [ 38  19   3  31   0   0   0   0   9   0]\n",
      " [ 29   6   7  26   0   0   2   0   3   0]\n",
      " [ 33  13   4  50   0   0   2   0   3   0]\n",
      " [ 14   4   3  11   1   0   0   1   7   0]\n",
      " [ 14   2   2  13   0   1   0   0   2   0]\n",
      " [ 38   6   1  35   0   0   5   0   4   0]\n",
      " [ 15   6   2  24   1   0   1   1   5   0]\n",
      " [ 26   2   1  23   0   0   0   0  15   0]\n",
      " [  9   1   0  11   0   0   0   0   2   0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.44s | valid accuracy    0.267 | f1_macro:    0.153  \n",
      "-----------------------------------------------------------\n",
      "[[125  24   7  14   0   0   4   4   4   0]\n",
      " [ 45  26  13   7   0   0   2   1   6   0]\n",
      " [ 35   9  15   7   3   0   2   1   1   0]\n",
      " [ 50  19  11  15   0   0   4   2   4   0]\n",
      " [ 18   5   7   4   1   0   0   1   5   0]\n",
      " [ 19   5   4   3   0   1   2   0   0   0]\n",
      " [ 46   6   2  16   0   0  10   1   8   0]\n",
      " [ 22   8   8   7   1   0   1   2   6   0]\n",
      " [ 32   1   3   8   0   0   1   1  21   0]\n",
      " [ 11   5   1   3   0   0   1   0   2   0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.41s | valid accuracy    0.281 | f1_macro:    0.171  \n",
      "-----------------------------------------------------------\n",
      "[[90 22  7 28  2  1 20  9  3  0]\n",
      " [28 25  6 17  2  0 10  4  8  0]\n",
      " [22  9  8 16  4  0  8  2  4  0]\n",
      " [29 14  6 27  1  0 14  7  7  0]\n",
      " [10  7  4  9  4  0  1  3  3  0]\n",
      " [ 8  5  2  4  1  2  8  3  1  0]\n",
      " [29  4  1 24  0  0 20  2  9  0]\n",
      " [12  7  2 13  1  0  6 10  4  0]\n",
      " [22  1  2 11  0  0  6  2 23  0]\n",
      " [ 7  5  0  7  0  0  2  0  2  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.38s | valid accuracy    0.272 | f1_macro:    0.204  \n",
      "-----------------------------------------------------------\n",
      "[[86 23  8 27  1  2 23  5  7  0]\n",
      " [23 29  7 14  2  0 12  2 11  0]\n",
      " [20 10 10 14  3  0 10  1  5  0]\n",
      " [24 19  7 25  1  0 16  6  7  0]\n",
      " [11  8  5  6  4  0  1  3  3  0]\n",
      " [ 8  5  2  5  1  2  8  1  2  0]\n",
      " [27  4  2 23  0  0 20  2 11  0]\n",
      " [10  8  4 12  1  0  7  7  6  0]\n",
      " [22  1  2 12  0  0  6  1 23  0]\n",
      " [ 6  7  0  4  0  0  2  0  4  0]]\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.40s | valid accuracy    0.268 | f1_macro:    0.199  \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val,f_macro = evaluate(valid_dataloader)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} | f1_macro: {:8.3f}  '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val,f_macro))\n",
    "    print('-' * 59)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14958d3aee5f1cad06795f787e54b96185c25fb40dfec723a5be941f3a531b8c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
