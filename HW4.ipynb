{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7346b726",
   "metadata": {},
   "source": [
    "<div align=\"center\" dir=\"rtl\">\n",
    "<h2>تمرین چهارم درس بازیابی پیشرفته‌ی اطلاعات</h2>\n",
    "<h3>گروه سی‌و‌سوم: اسامی</h3>\n",
    "</div>\n",
    "<div align=\"right\" dir=\"rtl\">\n",
    "  <br>\n",
    "    در این تمرین، تسک دسته‌بندی روی داده‌های گرد‌آوری شده از بوستان سعدی انجام می‌شود. ابتدا دیتاست مورد نیازمان از روی داده‌های خام که برای تمرین سوم جمع شده بود ساخته می‌شود، سپس قسمت پیش‌پردازش قرار دارد و در ادامه تسک روی داده‌ها انجام می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1bf1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت اول: آماده‌سازی دیتاست</h3>\n",
    "  <br>\n",
    "    در این قسمت فایل‌های داده‌های خام خوانده شده و دیتاست بیت به بیت برای کلاس‌بندی ایجاد می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c48ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk==3.3->hazm) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Using cached torchtext-0.12.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (1.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (4.62.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==1.11.0->torchtext) (4.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#import libraries for dataset generation\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "!pip install hazm\n",
    "!pip install torchtext\n",
    "from hazm import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2eabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = listdir('Boostan/')\n",
    "\n",
    "i = 0\n",
    "df = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "\n",
    "for p in all_files:\n",
    "    f = open('Boostan/' + p, 'r', encoding='utf8')\n",
    "    \n",
    "    has_poems = True\n",
    "    \n",
    "    while has_poems:\n",
    "        poem = ''\n",
    "        \n",
    "        while True:\n",
    "            s = f.readline()\n",
    "            if '$' in s:\n",
    "                has_poems = False\n",
    "                break\n",
    "            elif '_' in s:\n",
    "                break\n",
    "            \n",
    "            #age injaiim yani s ye mesra boode\n",
    "            beyt = s + f.readline()[:-1]\n",
    "            df.loc[i] = [beyt, p[:-4]]\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3783c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "باب ششم در قناعت\n",
      "3845\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178f294",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت دوم: پیش‌پردازش</h3>\n",
    "  <br>\n",
    "    در این قسمت یک نمونه‌ی پیش‌پردازش شده از دیتاست تهیه می‌شود. دیتاست پیش‌پردازش‌نشده هم در کنار آن حفظ می‌شود که امکان مقایسه‌ی نتیجه روی دو دیتاست وجود داشته باشد.\n",
    "    توضیحات دقیق این که هر گام از پیش‌پردازش شامل چه مواردی است، در تمرین سوم آورده شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0013c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for preprocessing\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eeef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac674b8",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی اول پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، برخی موارد در واژگان (مانند «گه» که همان «گاه» است) اصلاح می‌شوند. عملیات نرمالایز کردن و لمتایز کردن هم انجام داده می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e4ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def preprocess_1(beyt):\n",
    "    beyt = normalizer.normalize(beyt).replace(' ز ', ' از ').replace(' مر ', ' ') #normalize beshe va z beshe az\n",
    "    \n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        past_root = lemmatizer.lemmatize(word).split('#', 1)[0]\n",
    "        past_root = past_root.replace('آ', 'ا')\n",
    "        word_modified = word.replace(past_root, '')\n",
    "        \n",
    "        if word_modified != '':\n",
    "            if word_modified[0] == 'ب' and len(word_modified) < 4:\n",
    "                #yani fele maazi bode be forme ghadimi\n",
    "                beyt = beyt.replace(word, word[1:])\n",
    "        \n",
    "        if word.endswith('گه'):\n",
    "            beyt = beyt.replace(word, word[:-2]+'گاه')\n",
    "        \n",
    "        return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    df.at[i, 'beyt'] = preprocess_1(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2654",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی دوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، حالات باستانی افعال به حالت عادی تبدیل می‌شود تا افعال یک دست باشند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d2dfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        \n",
    "        if word[0] == 'م':\n",
    "            word_modified = 'ب' + word[1:]\n",
    "            present_root = lemmatizer.lemmatize(word_modified).split('#', 1)[-1]\n",
    "            present_root = present_root.replace('آ', 'ا')\n",
    "            word_modified = word_modified.replace(present_root, '')\n",
    "            \n",
    "            if word_modified == 'ب':\n",
    "                beyt = beyt.replace(word, 'ن'+word[1:])\n",
    "    return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):            \n",
    "    df.at[i, 'beyt'] = preprocess_2(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c6a20",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی سوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این مرحله، استاپ ووردها حذف شده و برخی واژگان خاص پیش‌پردازش می‌شوند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac84c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Exception.txt', encoding='utf8') as exc:\n",
    "    data = exc.read()\n",
    "special_verbs = json.loads(data)\n",
    "\n",
    "df_temp = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "stopwords = ['!','،','؟','ز','ار']+[normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
    "\n",
    "\n",
    "def preprocess_3(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    words = [t for t in words if t not in stopwords]\n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for t in words:\n",
    "        if t in special_verbs.keys():\n",
    "            lemmatized_tokens.append(t)\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(t).split('#')[0])\n",
    "    \n",
    "    lemmatized = ' '.join(lemmatized_tokens)\n",
    "            \n",
    "    return lemmatized, lemmatized_tokens\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pp = preprocess_3(df.at[i, 'beyt'])\n",
    "            \n",
    "    df.at[i, 'beyt'] = pp[0]\n",
    "    df_temp.at[i, 'beyt'] = pp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f83712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع سیم سنگ\n",
      "باب ششم در قناعت\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1a253aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'نپنداری قول معقول چو قانع شد سیم سنگ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test preprocessing\n",
    "preprocess_3(preprocess_2(preprocess_1('''نپنداری این قول معقول نیست\n",
    "چو قانع شدی سیم و سنگت یکی است''')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107e49e",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>تابع پیش‌پردازش تک‌بیت</h4>\n",
    "  <br>\n",
    "    حال یک تابع درست می‌کنیم که یک بیت را پیش‌پردازش کند تا بعدا هنگام دریافت ورودی سامانه از آن استفاده کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eba86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(beyt):\n",
    "    return preprocess_3(preprocess_2(preprocess_1(beyt)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0185d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "----\n",
      "نپنداری قول معقول چو قانع سیم سنگ\n",
      "----\n",
      "نپنداری قول معقول چو قانع سیم سنگ\n"
     ]
    }
   ],
   "source": [
    "#test preprocessing functions\n",
    "print(df_raw.at[2000, 'beyt'] + '\\n----')\n",
    "print(df.at[2000, 'beyt'] + '\\n----')\n",
    "print(preprocess(df_raw.at[2000, 'beyt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d212a",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت سوم: کلاس‌بندی با scikit</h3>\n",
    "  <br>\n",
    "    در این قسمت با ؟؟؟ کلاس‌بندی می‌کنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8f1fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for classification (1)\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "008c4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd3cb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, norm='l2')\n",
    "v = vectorizer.fit_transform(['نپنداری قول معقول چو قانع شد سیم سنگ', 'ممد ممد', 'ممد'])\n",
    "#v = augment(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bff98",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل اول:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                {اسم مدلتون رو اینجا بنویسید}\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c257641",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل دوم:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                {اسم مدلتون رو اینجا بنویسید}\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4155a",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t    <font size=4 color=green>\n",
    "\t\t\t    <br />\n",
    "                مدل سوم:\n",
    "                </font>\n",
    "                <font size=3>\n",
    "                مدل شبکه عصبی استفاده از کتابخانه\n",
    "                \n",
    "torchtext\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    </div>\n",
    "    <hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb9986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14958d3aee5f1cad06795f787e54b96185c25fb40dfec723a5be941f3a531b8c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
