{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7346b726",
   "metadata": {},
   "source": [
    "<div align=\"center\" dir=\"rtl\">\n",
    "<h2>تمرین چهارم درس بازیابی پیشرفته‌ی اطلاعات</h2>\n",
    "<h3>گروه سی‌و‌سوم: اسامی</h3>\n",
    "</div>\n",
    "<div align=\"right\" dir=\"rtl\">\n",
    "  <br>\n",
    "    در این تمرین، تسک دسته‌بندی روی داده‌های گرد‌آوری شده از بوستان سعدی انجام می‌شود. ابتدا دیتاست مورد نیازمان از روی داده‌های خام که برای تمرین سوم جمع شده بود ساخته می‌شود، سپس قسمت پیش‌پردازش قرار دارد و در ادامه تسک روی داده‌ها انجام می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1bf1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت اول: آماده‌سازی دیتاست</h3>\n",
    "  <br>\n",
    "    در این قسمت فایل‌های داده‌های خام خوانده شده و دیتاست بیت به بیت برای کلاس‌بندی ایجاد می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c48ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for dataset generation\n",
    "import pandas as pd\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2eabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = listdir('Boostan/')\n",
    "\n",
    "i = 0\n",
    "df = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "\n",
    "for p in all_files:\n",
    "    f = open('Boostan/' + p, 'r', encoding='utf8')\n",
    "    \n",
    "    has_poems = True\n",
    "    \n",
    "    while has_poems:\n",
    "        poem = ''\n",
    "        \n",
    "        while True:\n",
    "            s = f.readline()\n",
    "            if '$' in s:\n",
    "                has_poems = False\n",
    "                break\n",
    "            elif '_' in s:\n",
    "                break\n",
    "            \n",
    "            #age injaiim yani s ye mesra boode\n",
    "            beyt = s + f.readline()[:-1]\n",
    "            df.loc[i] = [beyt, p[:-4]]\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3783c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری این قول معقول نیست\n",
      "چو قانع شدی سیم و سنگت یکی است\n",
      "باب ششم در قناعت\n",
      "3845\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178f294",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت دوم: پیش‌پردازش</h3>\n",
    "  <br>\n",
    "    در این قسمت یک نمونه‌ی پیش‌پردازش شده از دیتاست تهیه می‌شود. دیتاست پیش‌پردازش‌نشده هم در کنار آن حفظ می‌شود که امکان مقایسه‌ی نتیجه روی دو دیتاست وجود داشته باشد.\n",
    "    توضیحات دقیق این که هر گام از پیش‌پردازش شامل چه مواردی است، در تمرین سوم آورده شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0013c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for preprocessing\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6eeef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac674b8",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی اول پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، برخی موارد در واژگان (مانند «گه» که همان «گاه» است) اصلاح می‌شوند. عملیات نرمالایز کردن و لمتایز کردن هم انجام داده می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32e4ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def preprocess_1(beyt):\n",
    "    beyt = normalizer.normalize(beyt).replace(' ز ', ' از ').replace(' مر ', ' ') #normalize beshe va z beshe az\n",
    "    \n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        past_root = lemmatizer.lemmatize(word).split('#', 1)[0]\n",
    "        past_root = past_root.replace('آ', 'ا')\n",
    "        word_modified = word.replace(past_root, '')\n",
    "        \n",
    "        if word_modified != '':\n",
    "            if word_modified[0] == 'ب' and len(word_modified) < 4:\n",
    "                #yani fele maazi bode be forme ghadimi\n",
    "                beyt = beyt.replace(word, word[1:])\n",
    "        \n",
    "        if word.endswith('گه'):\n",
    "            beyt = beyt.replace(word, word[:-2]+'گاه')\n",
    "        \n",
    "        return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    df.at[i, 'beyt'] = preprocess_1(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2654",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی دوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این گام، حالات باستانی افعال به حالت عادی تبدیل می‌شود تا افعال یک دست باشند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d2dfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    \n",
    "    for j in range(0, len(words)):\n",
    "        \n",
    "        word = words[j]\n",
    "        \n",
    "        if word[0] == 'م':\n",
    "            word_modified = 'ب' + word[1:]\n",
    "            present_root = lemmatizer.lemmatize(word_modified).split('#', 1)[-1]\n",
    "            present_root = present_root.replace('آ', 'ا')\n",
    "            word_modified = word_modified.replace(present_root, '')\n",
    "            \n",
    "            if word_modified == 'ب':\n",
    "                beyt = beyt.replace(word, 'ن'+word[1:])\n",
    "    return beyt\n",
    "\n",
    "for i in range(0, df.shape[0]):            \n",
    "    df.at[i, 'beyt'] = preprocess_2(df.at[i, 'beyt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c6a20",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>مرحله‌ی سوم پیش‌پردازش</h4>\n",
    "  <br>\n",
    "    در این مرحله، استاپ ووردها حذف شده و برخی واژگان خاص پیش‌پردازش می‌شوند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ac84c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Exception.txt', encoding='utf8') as exc:\n",
    "    data = exc.read()\n",
    "special_verbs = json.loads(data)\n",
    "\n",
    "df_temp = pd.DataFrame(columns=['beyt', 'baab'])\n",
    "stopwords = ['!','،','؟','ز','ار']+[normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
    "\n",
    "\n",
    "def preprocess_3(beyt):\n",
    "    words = word_tokenize(beyt)\n",
    "    words = [t for t in words if t not in stopwords]\n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for t in words:\n",
    "        if t in special_verbs.keys():\n",
    "            lemmatized_tokens.append(t)\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(t).split('#')[0])\n",
    "    \n",
    "    lemmatized = ' '.join(lemmatized_tokens)\n",
    "            \n",
    "    return lemmatized, lemmatized_tokens\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pp = preprocess_3(df.at[i, 'beyt'])\n",
    "            \n",
    "    df.at[i, 'beyt'] = pp[0]\n",
    "    df_temp.at[i, 'beyt'] = pp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f83712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "باب ششم در قناعت\n"
     ]
    }
   ],
   "source": [
    "print(df.at[2000, 'beyt'])\n",
    "print(df.at[2000, 'baab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1a253aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'نپنداری قول معقول چو قانع شد سیم سنگ'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test preprocessing\n",
    "preprocess_3(preprocess_2(preprocess_1('''نپنداری این قول معقول نیست\n",
    "چو قانع شدی سیم و سنگت یکی است''')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107e49e",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h4>تابع پیش‌پردازش تک‌بیت</h4>\n",
    "  <br>\n",
    "    حال یک تابع درست می‌کنیم که یک بیت را پیش‌پردازش کند تا بعدا هنگام دریافت ورودی سامانه از آن استفاده کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5eba86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(beyt):\n",
    "    return preprocess_3(preprocess_2(preprocess_1(beyt)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0185d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نپنداری این قول معقول نیست\n",
      "چو قانع شدی سیم و سنگت یکی است\n",
      "----\n",
      "نپنداری قول معقول چو قانع شد سیم سنگ\n",
      "----\n",
      "نپنداری قول معقول چو قانع شد سیم سنگ\n"
     ]
    }
   ],
   "source": [
    "#test preprocessing functions\n",
    "print(df_raw.at[2000, 'beyt'] + '\\n----')\n",
    "print(df.at[2000, 'beyt'] + '\\n----')\n",
    "print(preprocess(df_raw.at[2000, 'beyt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d212a",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    <h3>قسمت سوم: کلاس‌بندی با scikit</h3>\n",
    "  <br>\n",
    "    در این قسمت با ؟؟؟ کلاس‌بندی می‌کنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8f1fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for classification (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008c4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd3cb6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, norm='l2')\n",
    "v = vectorizer.fit_transform(['نپنداری قول معقول چو قانع شد سیم سنگ', 'ممد ممد', 'ممد'])\n",
    "#v = augment(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
